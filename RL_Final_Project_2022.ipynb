{"cells":[{"cell_type":"markdown","source":["## Installs and Imports"],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"gONZCUHuKvkc"}},{"cell_type":"markdown","source":["# Final Project - Reinforcements Learning \n","Hello dear students,<br> this is the template notebook. Please click on the \"File\" tab and then on \"Save a copy into drive\".\n","\n","---\n","<br>\n","\n","### Name and ID:\n","Student 1: Dror Meirovich 031846819\n","<br>\n","Student 2: N/A\n","<br><br>\n","# Goodluck!"],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"RllxV9GnKvkk"}},{"cell_type":"code","source":["!pip install highway-env\n","!pip install git+https://github.com/DLR-RM/stable-baselines3\n","!pip install tensorboardx gym pyvirtualdisplay\n","!apt-get install -y xvfb python-opengl ffmpeg\n","!git clone https://github.com/eleurent/highway-env.git 2> /dev/null\n","!git clone https://github.com/drormeir/HighwayRL.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"auwvgXe3AzSI","executionInfo":{"status":"ok","timestamp":1658827893116,"user_tz":-180,"elapsed":52941,"user":{"displayName":"Dror Meirovich","userId":"14310987058477987674"}},"outputId":"248434e8-96be-44b8-b09e-d36eb47c2af0","pycharm":{"name":"#%%\n"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting highway-env\n","  Downloading highway_env-1.5-py3-none-any.whl (103 kB)\n","\u001b[K     |████████████████████████████████| 103 kB 7.7 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from highway-env) (1.7.3)\n","Collecting pygame\n","  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n","\u001b[K     |████████████████████████████████| 21.8 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from highway-env) (1.3.5)\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from highway-env) (0.17.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from highway-env) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from highway-env) (1.21.6)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->highway-env) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->highway-env) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->highway-env) (0.16.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->highway-env) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->highway-env) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->highway-env) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->highway-env) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->highway-env) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->highway-env) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->highway-env) (2022.1)\n","Installing collected packages: pygame, highway-env\n","Successfully installed highway-env-1.5 pygame-2.1.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/DLR-RM/stable-baselines3\n","  Cloning https://github.com/DLR-RM/stable-baselines3 to /tmp/pip-req-build-c7nk92yu\n","  Running command git clone -q https://github.com/DLR-RM/stable-baselines3 /tmp/pip-req-build-c7nk92yu\n","Collecting gym==0.21\n","  Downloading gym-0.21.0.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 8.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.6.1a0) (1.21.6)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.6.1a0) (1.12.0+cu113)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.6.1a0) (1.3.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.6.1a0) (1.3.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.6.1a0) (3.2.2)\n","Requirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21->stable-baselines3==1.6.1a0) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3==1.6.1a0) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3==1.6.1a0) (4.1.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.6.1a0) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.6.1a0) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.6.1a0) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.6.1a0) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines3==1.6.1a0) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3==1.6.1a0) (2022.1)\n","Building wheels for collected packages: stable-baselines3, gym\n","  Building wheel for stable-baselines3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stable-baselines3: filename=stable_baselines3-1.6.1a0-py3-none-any.whl size=165533 sha256=cad9076c7893342d9622f096f8a6b195d3ca448d6ea5cc18e13601993bd7c4bc\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-zdv9yb1l/wheels/2b/88/65/5d0cb266b061107af8c518096240bea8578e9843716f79e4da\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616823 sha256=6839168905cbea91ae046079d02c6eaadbecf4fca78f19695e56af0ac89536ff\n","  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n","Successfully built stable-baselines3 gym\n","Installing collected packages: gym, stable-baselines3\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.17.3\n","    Uninstalling gym-0.17.3:\n","      Successfully uninstalled gym-0.17.3\n","Successfully installed gym-0.21.0 stable-baselines3-1.6.1a0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboardx\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.21.0)\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.21.6)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardx) (1.15.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym) (4.12.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym) (3.8.1)\n","Installing collected packages: tensorboardx, pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0 tensorboardx-2.5.1\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","Suggested packages:\n","  libgle3\n","The following NEW packages will be installed:\n","  python-opengl xvfb\n","0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 1,281 kB of archives.\n","After this operation, 7,687 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.11 [785 kB]\n","Fetched 1,281 kB in 1s (1,614 kB/s)\n","Selecting previously unselected package python-opengl.\n","(Reading database ... 155653 files and directories currently installed.)\n","Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n","Unpacking python-opengl (3.1.0+dfsg-1) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.11_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.11) ...\n","Setting up python-opengl (3.1.0+dfsg-1) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.11) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Cloning into 'highway-config'...\n","remote: Enumerating objects: 42, done.\u001b[K\n","remote: Counting objects: 100% (42/42), done.\u001b[K\n","remote: Compressing objects: 100% (40/40), done.\u001b[K\n","remote: Total 42 (delta 21), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (42/42), done.\n","Cloning into 'HighwayRL'...\n","remote: Enumerating objects: 48, done.\u001b[K\n","remote: Total 48 (delta 0), reused 0 (delta 0), pack-reused 48\u001b[K\n","Unpacking objects: 100% (48/48), done.\n","Checking out files: 100% (28/28), done.\n"]}]},{"cell_type":"code","source":["import gym\n","import highway_env\n","import sys\n","sys.path.insert(0, '/content/highway-env/scripts/')\n","from tqdm.notebook import trange\n","from utils import record_videos, show_videos\n","import numpy as np\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","from gym.utils import seeding\n","from gym import error, spaces, utils\n","gymlogger.set_level(40) # error only\n","import io\n","import base64\n","import os\n","import random\n","import matplotlib.pyplot as plt\n","import math\n","import glob\n","from pyvirtualdisplay import Display\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","import pygame\n","import json\n","import ast\n","%load_ext tensorboard\n","%matplotlib inline"],"metadata":{"id":"d-1qgImTBBuG","pycharm":{"name":"#%%\n"},"executionInfo":{"status":"ok","timestamp":1658821002254,"user_tz":-180,"elapsed":472,"user":{"displayName":"Dror Meirovich","userId":"14310987058477987674"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class ReplayBuffer:\n","    def __init__(self,\n","                 state_size, state_type,\n","                 action_size, action_type,\n","                 params=None,\n","                 seed: int = 0):\n","        if params is None:\n","            params = {'sample_every': 1, 'buffer_size': int(1e6), 'batch_size': 64}\n","        assert action_type == int or action_type == float\n","        self.state_size = state_size\n","        if action_type == int:  # DQN\n","            self.action_begin = 0\n","            self.action_end = action_size\n","            self.action_size = 1\n","        else:  # action_type == float --> DDPG\n","            self.action_begin = None\n","            self.action_end = None\n","            self.action_size = action_size\n","        self.state_type = state_type\n","        self.action_type = action_type\n","        self.buffer_size = params['buffer_size']\n","        self.batch_size = params['batch_size']\n","        self.current_len = 0\n","        self.sample_every = params['sample_every']\n","        self.last_sample_len = 0\n","        self.random = np.random.default_rng(seed=seed)\n","\n","        def np_empty(buffer_size, shape, d_type):\n","            if isinstance(shape, int):\n","                shape = (shape,)\n","            buffer_shape = (buffer_size,) + shape\n","            return np.empty(buffer_shape, dtype=d_type)\n","\n","        self.states = np_empty(self.buffer_size, self.state_size, d_type=self.state_type)\n","        self.actions = np_empty(self.buffer_size, self.action_size, d_type=self.action_type)\n","        self.rewards = np_empty(self.buffer_size, 1, d_type=np.float32)\n","        self.next_states = np_empty(self.buffer_size, self.state_size, d_type=self.state_type)\n","        self.dones = np_empty(self.buffer_size, 1, d_type=int)\n","\n","        self.res_states = np_empty(self.batch_size, self.state_size, d_type=self.state_type)\n","        self.res_actions = np_empty(self.batch_size, self.action_size, d_type=self.action_type)\n","        self.res_rewards = np_empty(self.batch_size, 1, d_type=np.float32)\n","        self.res_next_states = np_empty(self.batch_size, self.state_size, d_type=self.state_type)\n","        self.res_dones = np_empty(self.batch_size, 1, d_type=int)\n","\n","    def add_and_sample(self, state, action, reward, next_state, done):\n","        self.add(state, action, reward, next_state, done)\n","        if self.current_len < self.last_sample_len + self.sample_every:\n","            # No new mini batch\n","            return None\n","        self.last_sample_len = self.current_len\n","        return self.sample()\n","\n","    def sample(self):\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        active_len = min(self.current_len, self.buffer_size)\n","        if active_len < self.batch_size:\n","            # Not enough samples are available in memory\n","            return None\n","        indexes = self.random.choice(range(active_len), size=self.batch_size, replace=False)\n","        self.res_states[:] = self.states[indexes, :]\n","        self.res_actions[:] = self.actions[indexes, :]\n","        self.res_rewards[:] = self.rewards[indexes, :]\n","        self.res_next_states[:] = self.next_states[indexes, :]\n","        self.res_dones[:] = self.dones[indexes, :]\n","        return self.res_states, self.res_actions, self.res_rewards, self.res_next_states, self.res_dones\n","\n","    def add(self, state, action, reward, next_state, done):\n","        if self.action_type == int and self.action_size == 1:\n","            assert action >= self.action_begin\n","            assert action < self.action_end\n","        ind_pos = self.current_len % self.buffer_size\n","        self.states[ind_pos, :] = state\n","        self.actions[ind_pos][0] = action\n","        self.rewards[ind_pos][0] = reward\n","        self.next_states[ind_pos, :] = next_state\n","        self.dones[ind_pos][0] = done\n","        self.current_len += 1\n"],"metadata":{"id":"ToDxz8m8b3mX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch import autograd, optim, nn\n","\n","\n","class HighwayActorModelDQN(nn.Module):\n","    def __init__(self, state_size, action_size, model_params, seed: int = 0, pytorch_device=None):\n","        \"\"\"Initialize parameters and build model.\n","        Params\n","        ======\n","            state_size (tuple): Dimension of each state\n","            action_size (int): Dimension of each action\n","            seed (int): Random seed\n","            pytorch_device (str): pytorch cuda device name\n","        \"\"\"\n","        super().__init__()\n","        self.model_params = model_params\n","        ch_conv1, ch_conv2 = model_params['ch_conv1'], model_params['ch_conv2']\n","        fc0_out, fc1_out, fc2_out = model_params['fc0_out'], model_params['fc1_out'], model_params['fc2_out']\n","        self.seed = seed\n","        self.torch_seed = torch.manual_seed(seed)\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(self.state_size[0], ch_conv1, 5),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.Conv2d(ch_conv1, ch_conv2, 5),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2)\n","        )\n","        feature_dim = self.conv(autograd.Variable(torch.zeros(1, *self.state_size))).view(1, -1).size(1)\n","        self.base_stream = nn.Sequential(\n","            nn.Linear(feature_dim, fc0_out),\n","            nn.ReLU()\n","        )\n","        self.value_stream = nn.Sequential(\n","            nn.Linear(fc0_out, fc1_out),\n","            nn.ReLU(),\n","            nn.Linear(fc1_out, 1)\n","        )\n","        self.advantage_stream = nn.Sequential(\n","            nn.Linear(fc0_out, fc2_out),\n","            nn.ReLU(),\n","            nn.Linear(fc2_out, self.action_size)\n","        )\n","        self.apply(HighwayActorModelDQN.init_weights)\n","\n","        self.pytorch_device = pytorch_device\n","        if isinstance(pytorch_device, str):\n","            pytorch_device = pytorch_device.lower()\n","            if any(pytorch_device.startswith(device_name) for device_name in ['gpu', 'cuda']):\n","                if torch.cuda.is_available():\n","                    self.to(torch.device('cuda:0'))\n","\n","    def clone(self):\n","        ret = HighwayActorModelDQN(state_size=self.state_size, action_size=self.action_size,\n","                                   model_params=self.model_params,\n","                                   seed=self.seed, pytorch_device=self.pytorch_device)\n","        ret.soft_update_from_local(self, 1.0)\n","        return ret\n","\n","    @property\n","    def my_device(self):\n","        return next(self.parameters()).device\n","\n","    def act(self, state):\n","        self.eval()\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(self.my_device)\n","        with torch.no_grad():\n","            action_values = self(state).cpu().data.numpy()\n","        return int(np.argmax(action_values))\n","\n","    def q_targets(self, rewards, gamma, dones, next_states):\n","        self.eval()\n","        rewards = torch.from_numpy(rewards).float().to(self.my_device)\n","        dones = torch.from_numpy(dones).int().to(self.my_device)\n","        next_states = torch.from_numpy(next_states).float().to(self.my_device)\n","        # Get max predicted Q values (for next states) from target model\n","        q_targets_next = self(next_states).detach().max(1)[0].unsqueeze(1)\n","        # Compute Q targets for current states\n","        q_targets = rewards + gamma * (1 - dones) * q_targets_next\n","        return q_targets\n","\n","    def q_expected(self, states, actions):\n","        self.train()\n","        # Get expected Q values from local model\n","        states = torch.from_numpy(states).float().to(self.my_device)\n","        actions = torch.from_numpy(actions).long().to(self.my_device)\n","        q_expected = self(states).gather(1, actions)\n","        return q_expected\n","\n","    def soft_update_from_local(self, local_q_network, local_tau_weight):\n","        for target_param, local_param in zip(self.parameters(), local_q_network.parameters()):\n","            target_param.data.copy_(local_tau_weight * local_param.data + (1.0 - local_tau_weight) * target_param.data)\n","\n","    def forward(self, state):\n","        features = self.conv(state)\n","        features = features.view(features.size(0), -1)\n","        features = self.base_stream(features)\n","        values = self.value_stream(features)\n","        advantages = self.advantage_stream(features)\n","        q_vals = values + (advantages - advantages.mean())\n","        return q_vals\n","\n","    def save(self, filename):\n","        shutil.rmtree(filename, ignore_errors=True)  # avoid file not found error\n","        torch.save(self.state_dict(), filename)\n","\n","    def load(self, filename):\n","        self.load_state_dict(torch.load(filename))\n","\n","    @staticmethod\n","    def init_weights(m):\n","        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","            torch.nn.init.xavier_uniform_(m.weight)\n","            m.bias.data.fill_(0.01)\n","\n","\n","class OptimizerDQN:\n","    def __init__(self, local_q_network, lr):\n","        self.optimizer = optim.Adam(local_q_network.parameters(), lr=lr)\n","\n","    def step(self, q_expected, q_targets):\n","        if isinstance(q_targets, list):\n","            q_targets = torch.min(torch.stack(q_targets), dim=0)[0]  # [0] to get the values\n","        # Compute loss\n","        loss = F.mse_loss(q_expected, q_targets)\n","        # Minimize the loss\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def save(self, filename):\n","        shutil.rmtree(filename, ignore_errors=True)  # avoid file not found error\n","        torch.save(self.optimizer.state_dict(), filename)\n","\n","    def load(self, filename):\n","        self.optimizer.load_state_dict(torch.load(filename))\n","\n"],"metadata":{"id":"JG8kQZBQb3k2","executionInfo":{"status":"ok","timestamp":1658821686475,"user_tz":-180,"elapsed":2773,"user":{"displayName":"Dror Meirovich","userId":"14310987058477987674"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","class HighwayAgentDQN:\n","    def __init__(self, env,\n","                 seed=0,\n","                 eps_greedy_min=0.01, eps_greedy_decay=0.981,\n","                 replay_buffer_size: int = int(1e4),\n","                 replay_batch_size: int = 40,\n","                 train_every_episode_steps=2,\n","                 gamma=0.987,\n","                 local_tau_weight=0.003,\n","                 num_targets: int = 1,\n","                 lr=5e-4,\n","                 ch_conv1=16,\n","                 ch_conv2=64,\n","                 fc0_out=180,\n","                 fc1_out=180,\n","                 fc2_out=200,\n","                 reward_power=1.5,\n","                 pytorch_device=None,\n","                 verbose_level=1):\n","\n","        self.eps_greedy = 1.0\n","        self.eps_greedy_min = eps_greedy_min\n","        self.eps_greedy_decay = eps_greedy_decay\n","        self.action_size = env.action_size\n","        self.state_size = env.state_size\n","        self.env_name = env.env_name\n","        self.reward_power = reward_power\n","        self.random = np.random.default_rng(seed=seed)\n","        model_params = {'ch_conv1': ch_conv1, 'ch_conv2': ch_conv2,\n","                        'fc0_out': fc0_out, 'fc1_out': fc1_out, 'fc2_out': fc2_out}\n","        self.local_q_network = HighwayActorModelDQN(state_size=self.state_size, action_size=self.action_size,\n","                                                    model_params=model_params,\n","                                                    seed=seed, pytorch_device=pytorch_device)\n","        self.target_q_network = [\n","            HighwayActorModelDQN(state_size=self.state_size, action_size=self.action_size, model_params=model_params,\n","                                 seed=seed + i_target, pytorch_device=pytorch_device)\n","            for i_target in range(num_targets)]\n","\n","        replay_buffer_params = {'sample_every': train_every_episode_steps,\n","                                'buffer_size': replay_buffer_size, 'batch_size': replay_batch_size}\n","        self.replay_buffer = ReplayBuffer(state_size=self.state_size, state_type=np.uint8,\n","                                          action_size=self.action_size, action_type=int,\n","                                          params=replay_buffer_params,\n","                                          seed=seed)\n","        self.optimizer = OptimizerDQN(self.local_q_network, lr=lr)\n","        self.ind_next_step_in_episode = 0\n","        self.gamma = gamma\n","        self.local_tau_weight = local_tau_weight\n","        self.verbose_level = verbose_level\n","        if self.verbose_level > 0:\n","            print('Initializing HighwayAgentDQN with:')\n","            print(f'Epsilon Greedy: min={self.eps_greedy_min} decay={self.eps_greedy_decay}')\n","            print(f'Gamma = {self.gamma}')\n","            print(f'tau = {self.local_tau_weight}')\n","            print(f'Replay Buffer: {replay_buffer_params}')\n","            print(f'Learning rate = {lr:4.2e}')\n","            print(f'Model params = {model_params}')\n","        self.params_dict = {'replay_buffer': replay_buffer_params, 'Q_network': model_params, 'gamma': gamma,\n","                            'tau': self.local_tau_weight, 'learning_rate': lr, 'epsilon_min': self.eps_greedy_min,\n","                            'epsilon_decay': self.eps_greedy_decay, 'reward_power': self.reward_power, 'seed': seed}\n","\n","    def act(self, state, training):\n","        \"\"\"Returns actions for given state as per current policy.\n","\n","        Params\n","        ======\n","            state (array_like): current state\n","            training (bool): true = epsilon-greedy action selection\n","        \"\"\"\n","        if training:\n","            if self.random.random() < self.eps_greedy:\n","                # explore the environment using random move\n","                return int(self.random.choice(self.action_size))\n","        # exploit the agent knowledge\n","        return self.local_q_network.act(state)\n","\n","    def step(self, state, action, reward, next_state, done, info):\n","        # ind_curr_step_in_episode = self.ind_next_step_in_episode\n","        if done:\n","            # episode can end because reaching maximum steps, hence set done == True iff crash\n","            done = info['crashed']\n","            # preparing for next episode...\n","            self.ind_next_step_in_episode = 0\n","            self.eps_greedy = max(self.eps_greedy * self.eps_greedy_decay, self.eps_greedy_min)\n","        else:\n","            self.ind_next_step_in_episode += 1\n","        # minimal velocity has reward of 0.7 and maximal velocity has reward close to 1.0\n","        # hence, encourage agent achieve higher velocity\n","        reward = math.pow(reward, self.reward_power)\n","        state = state.astype(np.uint8)\n","        next_state = next_state.astype(np.uint8)\n","        experiences = self.replay_buffer.add_and_sample(state, action, reward, next_state, done)\n","        if experiences is None:\n","            return\n","        states, actions, rewards, next_states, dones = experiences\n","\n","        q_targets = [q_target.q_targets(rewards, self.gamma, dones, next_states) for q_target in self.target_q_network]\n","\n","        q_expected = self.local_q_network.q_expected(states, actions)\n","\n","        self.optimizer.step(q_expected, q_targets)\n","\n","        for q_target in self.target_q_network:\n","            q_target.soft_update_from_local(self.local_q_network, local_tau_weight=self.local_tau_weight)\n","\n","    def save_param_dict(self, filename):\n","        with open(filename, 'w') as f:\n","            json.dump(self.params_dict, f)\n","\n","    def load_param_dict(self, filename):\n","        with open(filename, 'r') as f:\n","            self.params_dict = json.load(f)\n"],"metadata":{"id":"Gxz1S1JJfOf6","executionInfo":{"status":"ok","timestamp":1658821872165,"user_tz":-180,"elapsed":526,"user":{"displayName":"Dror Meirovich","userId":"14310987058477987674"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6Z1XvYZjfOeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"AaxsFvoRfOdm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Display utils\n","The cell below contains the video display configuration. No need to make changes here."],"metadata":{"id":"I_CiTInf6Vrq","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBL7zGQU3Y6t","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n","    !bash ../xvfb start\n","    %env DISPLAY=:1\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment \n","and displaying it.\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"]},{"cell_type":"markdown","source":["## EX1 - Highway-Env - Grayscale Image - Easy\n"],"metadata":{"id":"OyCjqcmF1czs","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["#=============== DO NOT DELETE ===============\n","file = open('/content/HighwayRL/highway-config/config_ex1.txt', 'r')\n","contents = file.read()\n","config1 = ast.literal_eval(contents)\n","file.close()\n","# ============================================\n","\n","env = gym.make(\"highway-fast-v0\")\n","env.configure(config1)\n","obs = env.reset()\n","for j in range(10):\n","    obs, _, _, _ = env.step(0)\n","\n","    _, axes = plt.subplots(ncols=4, figsize=(12, 5))\n","    for i, ax in enumerate(axes.flat):\n","        ax.imshow(obs[i, ...].T, cmap=plt.get_cmap('gray'))\n","    \n","\n","env = wrap_env(env)\n","env.reset()\n","done = False\n","iter = 0\n","\n","while (iter < 10) or not done:\n","  if done:\n","    break\n","  iter +=1\n","  action = env.action_space.sample()\n","  observation, reward, done, _ = env.step(action)\n","  screen = env.render(mode='rgb_array')\n","  plt.imshow(screen)\n","  print(f'iteration: {iter}, action: {action}, reward: {reward}, done: {done}')"],"metadata":{"id":"U1WS42MX1afS","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env.close()\n","show_video()"],"metadata":{"id":"cWjls4bK1xch","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# EX2 - Highway-Env - Grayscale Image - Medium"],"metadata":{"id":"r-p6UQjcOpCi","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["#=============== DO NOT DELETE ===============\n","file = open('/content/HighwayRL/highway-config/config_ex2.txt', 'r')\n","contents = file.read()\n","config2 = ast.literal_eval(contents)\n","file.close()\n","# ============================================\n","\n","env = gym.make(\"highway-fast-v0\")\n","env.configure(config2)\n","\n","obs = env.reset()\n","for j in range(10):\n","    sam_act = env.action_space.sample()\n","    obs, _, _, _ = env.step(sam_act)\n","\n","    _, axes = plt.subplots(ncols=4, figsize=(12, 5))\n","    for i, ax in enumerate(axes.flat):\n","        ax.imshow(obs[i, ...].T, cmap=plt.get_cmap('gray'))\n","    \n","env = wrap_env(env)\n","env.reset()\n","done = False\n","iter = 0\n","while (iter < 40) or not done:\n","  if done:\n","    break\n","  iter +=1\n","  action = env.action_space.sample()\n","  observation, reward, done, _ = env.step(action)\n","  screen = env.render(mode='rgb_array')\n","  plt.imshow(screen)\n","  print(f'iteration: {iter}, action: {action}, reward: {reward}, done: {done}')"],"metadata":{"id":"vGHHlv0DTosQ","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Play video"],"metadata":{"id":"9wjUAPoDQRsT","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["env.close()\n","show_video()"],"metadata":{"id":"4Yz-gc8cQP5b","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ex3 - Super Highway Agent"],"metadata":{"id":"nFJj30xsOim9","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["#=============== DO NOT DELETE ===============\n","file = open('/content/HighwayRL/highway-config/config_ex3.txt', 'r')\n","contents = file.read()\n","config3 = ast.literal_eval(contents)\n","file.close()\n","# ============================================"],"metadata":{"id":"Gy9rqT1mOjJg","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ex3 - Highway-Env"],"metadata":{"id":"sDweVhUOPShT","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["env = gym.make(\"highway-fast-v0\")\n","env.configure(config3) \n","env = wrap_env(env)\n","env.reset()\n","done = False\n","iter = 0\n","while (iter < 10) or not done:\n","  if done:\n","    break\n","  iter +=1\n","  action = env.action_space.sample()\n","  observation, reward, done, _ = env.step(action)\n","  screen = env.render(mode='rgb_array')\n","  plt.imshow(screen)\n","  print(f'iteration: {iter}, action: {action}, reward: {reward}, done: {done}')"],"metadata":{"id":"O2wDH0A3Pa5G","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Play video"],"metadata":{"id":"tp37JKNZRyO4","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["env.close()\n","show_video()"],"metadata":{"id":"p3rVoJyKC18_","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ex3 - Merge-Env"],"metadata":{"id":"N76BmfKzR69N","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["env = gym.make(\"merge-v0\")\n","env.configure(config3) \n","env = wrap_env(env)\n","env.reset()\n","done = False\n","iter = 0\n","\n","while (iter < 10) or not done:\n","  if done:\n","    break\n","  iter +=1\n","  action = env.action_space.sample()\n","  observation, reward, done, _ = env.step(action)\n","  screen = env.render(mode='rgb_array')\n","  plt.imshow(screen)\n","  print(f'iteration: {iter}, action: {action}, reward: {reward}, done: {done}')"],"metadata":{"id":"Br2azG32R9WD","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Play video\n"],"metadata":{"id":"D11YUSaaR7gm","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["env.close()\n","show_video()"],"metadata":{"id":"qB_-zbHWR-Dt","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ex3 - Roundabout-Env"],"metadata":{"id":"PNMGyTc7R-pK","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["env = gym.make(\"roundabout-v0\")\n","env.configure(config3) \n","env = wrap_env(env)\n","env.reset()\n","done = False\n","iter = 0\n","while (iter < 10) or not done:\n","  if done:\n","    break\n","  iter +=1\n","  action = env.action_space.sample()\n","  observation, reward, done, _ = env.step(action)\n","  screen = env.render(mode='rgb_array')\n","  plt.imshow(screen)\n","  print(f'iteration: {iter}, action: {action}, reward: {reward}, done: {done}')"],"metadata":{"id":"u1-sW6nxR-0u","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Play video"],"metadata":{"id":"6nh02olRR-9p","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["env.close()\n","show_video()"],"metadata":{"id":"1r_2kJw_R_GP","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"RL_Final_Project_2022.ipynb","provenance":[{"file_id":"1SB4Rlbcw6TnSQOXpS1nITgM7lrfOD974","timestamp":1658680918635},{"file_id":"1Q10FbZDcgMGvl0PvZcVfMTK9S17TELU8","timestamp":1653934963764},{"file_id":"1Y0eDRyJizfrUlUI5SnG8pwOihCshU7GE","timestamp":1653326492036},{"file_id":"1pgkjtQxL7zbrwi7YCtf2psJxxF2n8qOt","timestamp":1649791677693},{"file_id":"1Ni9OvGuUpCqMCkJbpMyQORfeBLzc2aD5","timestamp":1649166396090},{"file_id":"1Y40xnAyva4k0Dlt4_3Blf5VQJWUQOesJ","timestamp":1638275852967},{"file_id":"16gZuQlwxmxR5ZWYLZvBeq3bTdFfb1r_6","timestamp":1597649365945},{"file_id":"10S5_78iJovmDCVtCb75H_iB47hYBY2eV","timestamp":1525868817676}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}